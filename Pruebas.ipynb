{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c69ce608-e069-455b-9093-025ab209f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d53e490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-714657f9-a907-4aba-9fa7-b2f1a2659057-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d81b9256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OutputParsers\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88461eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into italian:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Templates\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "# result\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c9ada4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chaining together components with LCEL\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi!\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f6eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message persistence\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd09c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Si tienes dolor de cabeza, te recomendaría descansar en un lugar tranquilo y oscuro, beber suficiente agua, y si es necesario, tomar algún analgésico como el paracetamol siguiendo las indicaciones del envase. También puedes intentar aplicar compresas frías en la frente o cuello para aliviar el dolor. Si el dolor persiste o empeora, es recomendable consultar a un médico.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Que hago si tengo dolor de cabeza?.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d073eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab57d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Cargar la página web\n",
    "loader = WebBaseLoader(\"https://www.promtior.ai/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Dividir el texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Crear embeddings y vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# 4. Crear el chain de conversación\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 5. Configurar LangGraph para memoria\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    # Obtener la última pregunta\n",
    "    question = state[\"messages\"][-1].content\n",
    "    # Obtener historial previo\n",
    "    chat_history = [(msg.content, \"\") for msg in state[\"messages\"][:-1]]\n",
    "    \n",
    "    # Consultar usando el chain que incluye la búsqueda en la web\n",
    "    response = chain({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    return {\"messages\": [HumanMessage(content=response[\"answer\"])]}\n",
    "\n",
    "# Configurar el grafo\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.set_entry_point(\"model\")\n",
    "workflow.set_finish_point(\"model\")\n",
    "\n",
    "# Compilar con memoria\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# 6. Función para chatear\n",
    "def chat_with_website(query, config={\"configurable\": {\"thread_id\": \"promtior_chat\"}}):\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = app.invoke(\n",
    "        {\"messages\": input_messages},\n",
    "        config\n",
    "    )\n",
    "    return output[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a30b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csosa\\AppData\\Local\\Temp\\ipykernel_27820\\82455575.py:51: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promtior ofrece una variedad de servicios, incluyendo la aceleración de la adopción de GenAI, soluciones personalizadas de GenAI desde el descubrimiento y desarrollo hasta la implementación, consultoría en la adopción de GenAI, y un departamento de GenAI como servicio que impulsa el rendimiento del equipo con talentos expertos en GenAI. Además, Promtior también ofrece soluciones completas de GenAI que incluyen análisis predictivo, automatización inteligente, entre otros.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "respuesta = chat_with_website(\"¿Qué servicios ofrece Promtior?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59145d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promtior ofrece servicios en diversas áreas, incluyendo servicios en el departamento de GenAI, consultoría de adopción de GenAI, y la transformación del negocio con Generative AI. En el departamento de GenAI, ofrecen servicios como la optimización de procesos de flujo de trabajo, la reducción de errores manuales, la generación automática y revisión de código, y la automatización de pruebas de software, entre otros. En el área de consultoría de adopción de GenAI, se enfocan en transformar datos complejos en ideas accionables, conectando la inteligencia artificial con los objetivos comerciales. Y en la transformación del negocio con Generative AI, ofrecen servicios como la automatización de tareas repetitivas, la personalización de comunicaciones con clientes, y la predicción de las necesidades de los clientes, entre otros.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta de seguimiento\n",
    "respuesta = chat_with_website(\"¿Puedes darme más detalles sobre esos servicios?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b09c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sé la respuesta a esa pregunta.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta de seguimiento\n",
    "respuesta = chat_with_website(\"Cuando nació Trump?\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "223cd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tengo información adicional además de la proporcionada sobre Promtior en este contexto.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta de seguimiento\n",
    "respuesta = chat_with_website(\"Solo sabes informacion de la pagina de Promtior? O sabes algo mas?\")\n",
    "print(respuesta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
